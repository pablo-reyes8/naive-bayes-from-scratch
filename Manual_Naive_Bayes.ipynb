{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We will implement a `NaiveBayes` classifier for categorical features in pure Python, mimicking the interface of scikit-learn. Our `NaiveBayes` class will expose two primary methods:\n",
    "\n",
    "1. **`fit(X, y, α=1)`**  \n",
    "   - **Goal**: Estimate the class priors $P(Y=i)$ and the conditional likelihoods $P(X_j = v \\mid Y = i)$ for each feature $X_j$ and each class $i$.\n",
    "   - **Class Priors**  \n",
    "     Let $N(i)$ be the count of samples in class $i$, and $N_{\\text{tot}}$ the total number of samples. Then\n",
    "     $$\n",
    "       P(Y=i) = \\frac{N(i)}{N_{\\text{tot}}}.\n",
    "     $$\n",
    "   - **Laplace‐smoothed Likelihoods**  \n",
    "     For each feature $X_j$ taking value $v$ and each class $i$, let\n",
    "     - $N(v, i)$ = number of times $X_j=v$ among samples with $Y=i$,\n",
    "     - $k_j$ = number of distinct categories of feature $X_j$,\n",
    "     - $\\alpha$ = smoothing parameter (default $\\alpha=1$).  \n",
    "     \n",
    "     Then\n",
    "     $$\n",
    "       P(X_j = v \\mid Y = i)\n",
    "       = \\frac{N(v, i) + \\alpha}{N(i) + \\alpha\\,k_j}.\n",
    "     $$\n",
    "     This ensures no probability is zero, so the product of likelihoods remains nonzero.\n",
    "\n",
    "   All estimated probabilities are stored in dictionaries:\n",
    "   ```python\n",
    "   self.prior      # {class_i: P(Y=i)}\n",
    "   self.likelihood # {(j, i): {v: P(X_j=v | Y=i)}}\n",
    "\n",
    "   \n",
    "2. **`predict(x_new)`**  \n",
    "- **Goal**: Compute the posterior score for each class $i$ given a new sample $\\mathbf{x} = (x_1, \\dots, x_n)$ and return the class with the highest score.  \n",
    "- **Posterior (log‐space)**  \n",
    "  Since  \n",
    "  $$\n",
    "    P(Y=i \\mid \\mathbf{x})\n",
    "    \\propto P(Y=i)\\,\\prod_{j=1}^n P(X_j = x_j \\mid Y=i),\n",
    "  $$  \n",
    "  we work in log‐space to avoid underflow:  \n",
    "  $$\n",
    "    \\log P(Y=i \\mid \\mathbf{x})\n",
    "    = \\log P(Y=i)\n",
    "    + \\sum_{j=1}^n \\log P(X_j = x_j \\mid Y=i).\n",
    "  $$  \n",
    "- **Decision Rule**  \n",
    "  Return  \n",
    "  $$\n",
    "    \\widehat{y} = \\arg\\max_i \\Bigl\\{\\,\\log P(Y=i) + \\sum_{j=1}^n \\log P(X_j = x_j \\mid Y=i)\\Bigr\\}.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets import the necesary librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlook  temp  humidity  windy  play\n",
       "0         2     1         0      0     0\n",
       "1         2     1         0      1     0\n",
       "2         0     1         0      0     1\n",
       "3         1     2         0      0     1\n",
       "4         1     0         1      0     1\n",
       "5         1     0         1      1     0\n",
       "6         0     0         1      1     1\n",
       "7         2     2         0      0     0\n",
       "8         2     0         1      0     1\n",
       "9         1     2         1      0     1\n",
       "10        2     2         1      1     1\n",
       "11        0     2         0      1     1\n",
       "12        0     1         1      0     1\n",
       "13        1     2         0      1     0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennis = pd.read_csv('Bases de datos\\\\tennis.csv')\n",
    "encoders = {}\n",
    "\n",
    "for i in tennis.columns:\n",
    "    le = LabelEncoder()\n",
    "    tennis[i] = le.fit_transform(tennis[i])\n",
    "    encoders[i] = le \n",
    "\n",
    "tennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in tennis.columns if i != 'play']\n",
    "X = tennis[labels]\n",
    "y = tennis['play']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self , X , y , alpha = 1 , parameters = False):\n",
    "        \"\"\"\n",
    "        Estimate class priors and feature likelihoods for a categorical Naive Bayes model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame\n",
    "            Feature matrix with categorical columns.\n",
    "        y : pandas.Series\n",
    "            Target labels corresponding to each row of X.\n",
    "        alpha : float, default=1\n",
    "            Laplace smoothing parameter.\n",
    "        parameters : bool, default=False\n",
    "            If True, return the raw likelihood, prior, and class counts.\n",
    "\n",
    "        Attributes set on self\n",
    "        ----------------------\n",
    "        likelihood : dict\n",
    "            Nested dict mapping each class to a dict of feature→{value: P(value|class)}.\n",
    "        prior : dict\n",
    "            Mapping from class to P(class).\n",
    "        cols : int\n",
    "            Number of feature columns.\n",
    "        labels : list\n",
    "            List of original column names from X.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = [i for i in X.columns]\n",
    "        clases_totales , prior , likelihood= {} ,{} , {}\n",
    "        combinada = X.copy()\n",
    "        combinada['y'] = y\n",
    "\n",
    "        # Build a dict of total counts per class\n",
    "        for i in y:\n",
    "            if i not in clases_totales:\n",
    "                clases_totales[i] = y.eq(i).sum()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # Compute class priors: count_of_class / total_samples\n",
    "        for i in clases_totales:\n",
    "            prior[i] = clases_totales[i]/y.shape[0]\n",
    "            likelihood[i] = {}\n",
    "\n",
    "        # Compute likelihoods with Laplace smoothing\n",
    "        for i in prior:\n",
    "            # Filter rows belonging to class i\n",
    "            df1 = combinada[combinada['y'] == i]\n",
    "            for k in range(X.shape[1]):\n",
    "                posibles = X.iloc[:, k].unique()\n",
    "                n_posibles = len(posibles)\n",
    "\n",
    "                # Count occurrences of each category value within class i\n",
    "                dic = df1.iloc[:, k].value_counts().to_dict()\n",
    "                # Apply Laplace smoothing:\n",
    "                # (count(value, class) + alpha) / (count(class) + alpha * number_of_categories)\n",
    "                dic_dividido = {valor: (dic.get(valor, 0) + alpha) / (clases_totales[i] + alpha * n_posibles) for valor in posibles}\n",
    "                likelihood[i][labels[k]] = dic_dividido\n",
    "\n",
    "        # Store the computed parameters\n",
    "        self.likelihood = likelihood\n",
    "        self.prior = prior\n",
    "        self.cols = X.shape[1]\n",
    "        self.labels = labels\n",
    "\n",
    "        # Optionally return raw parameters for inspection\n",
    "        if parameters:\n",
    "            return likelihood , prior , clases_totales\n",
    "\n",
    "    def predict(self , l:list):\n",
    "        \"\"\"\n",
    "        Predict the class label for a single observation using the trained Naive Bayes model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        l : list\n",
    "            A list of feature values for the new observation. Length must match the number of features used in training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (predicted_class, class_log_probabilities)\n",
    "            - predicted_class: the class with the highest posterior log-probability\n",
    "            - class_log_probabilities: dict mapping each class to its computed log-posterior\n",
    "        \"\"\"\n",
    "        prob_finales = {}\n",
    "        probabilidad = 0\n",
    "        c, final = float('-inf') , None\n",
    "\n",
    "        # Check that input has correct number of features\n",
    "        if len(l) != self.cols:\n",
    "            return f'Model was trained with a different number of features'\n",
    "        else:\n",
    "\n",
    "            # Compute log-posterior for each class\n",
    "            for i in self.prior:\n",
    "                # Sum log-likelihoods for each feature\n",
    "                for etiquetas , nuevo in zip(self.labels, l):\n",
    "                    if nuevo not in self.likelihood[i][etiquetas]:\n",
    "                        # If unseen value, add a small log-probability to avoid zero\n",
    "                        probabilidad += np.log(9e-11)\n",
    "                    else:\n",
    "                        probabilidad += np.log(self.likelihood[i][etiquetas][nuevo])\n",
    "\n",
    "                # Add log-prior to get log-posterior\n",
    "                prob_finales[i] = probabilidad + np.log(self.prior[i])\n",
    "                probabilidad = 0\n",
    "\n",
    "        # Select the class with the highest log-posterior\n",
    "        for i , k in prob_finales.items():\n",
    "            if k > c:\n",
    "                final = i\n",
    "                c = k\n",
    "            else:\n",
    "                pass\n",
    "        return final , prob_finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes()\n",
    "verosimilitud , priors , frecuencia = model.fit(X,y , parameters= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets check the frecuency of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5, 1: 9}\n"
     ]
    }
   ],
   "source": [
    "print(frecuencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets see the prior probabilitys of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.35714285714285715, 1: 0.6428571428571429}\n"
     ]
    }
   ],
   "source": [
    "print(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally let's see the Likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'outlook': {2: 0.5, 0: 0.125, 1: 0.375},\n",
       "  'temp': {1: 0.375, 2: 0.375, 0: 0.25},\n",
       "  'humidity': {0: 0.7142857142857143, 1: 0.2857142857142857},\n",
       "  'windy': {0: 0.42857142857142855, 1: 0.5714285714285714}},\n",
       " 1: {'outlook': {2: 0.25, 0: 0.4166666666666667, 1: 0.3333333333333333},\n",
       "  'temp': {1: 0.25, 2: 0.4166666666666667, 0: 0.3333333333333333},\n",
       "  'humidity': {0: 0.36363636363636365, 1: 0.6363636363636364},\n",
       "  'windy': {0: 0.6363636363636364, 1: 0.36363636363636365}}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Interpretation of the Likelihood Dictionary\n",
    "\n",
    "\n",
    "\n",
    "- **Top-level keys** (`0`, `1`) are the class labels $Y=0$ and $Y=1$.  \n",
    "- **Feature keys** (`'outlook'`, `'temp'`, `'humidity'`, `'windy'`) list the conditional probabilities for each feature.  \n",
    "- **Values** inside each feature map represent $P(X_j = v \\mid Y=\\text{class})$.\n",
    "\n",
    "---\n",
    "\n",
    "### Class = 0\n",
    "\n",
    "- **outlook**  \n",
    "  - 2 → 0.50  \n",
    "  - 0 → 0.125  \n",
    "  - 1 → 0.375  \n",
    "  _Half of the class 0 samples have outlook=2; only 12.5% have outlook=0._\n",
    "\n",
    "- **temp**  \n",
    "  - 1 → 0.375  \n",
    "  - 2 → 0.375  \n",
    "  - 0 → 0.25  \n",
    "  _Temperatures 1 and 2 are equally common; temp=0 is less frequent._\n",
    "\n",
    "- **humidity**  \n",
    "  - 0 → 0.714  \n",
    "  - 1 → 0.286  \n",
    "  _Low humidity strongly indicates class 0 (about 71% of samples)._\n",
    "\n",
    "- **windy**  \n",
    "  - 0 → 0.429  \n",
    "  - 1 → 0.571  \n",
    "  _Slightly more class 0 samples are windy (57%)._\n",
    "\n",
    "---\n",
    "\n",
    "### Class = 1\n",
    "\n",
    "- **outlook**  \n",
    "  - 0 → 0.417  \n",
    "  - 1 → 0.333  \n",
    "  - 2 → 0.25  \n",
    "  _Overcast (0) is most common; rainy (2) is least common for class 1._\n",
    "\n",
    "- **temp**  \n",
    "  - 2 → 0.417  \n",
    "  - 0 → 0.333  \n",
    "  - 1 → 0.25  \n",
    "  _Higher temperatures (2) favor class 1._\n",
    "\n",
    "- **humidity**  \n",
    "  - 1 → 0.636  \n",
    "  - 0 → 0.364  \n",
    "  _High humidity strongly suggests class 1._\n",
    "\n",
    "- **windy**  \n",
    "  - 0 → 0.636  \n",
    "  - 1 → 0.364  \n",
    "  _Not windy favors class 1._\n",
    "\n",
    "---\n",
    "\n",
    "By comparing these tables, you can see which feature values most strongly differentiate the two classes. Features like **humidity** (with large probability gaps) will dominate the Naive Bayes decision when you sum log-probabilities for a new observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets predict a new sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, {0: -5.209121787743566, 1: -4.102643365036796})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([1,0 , 1 , 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s important to remember that these values are **log-probabilities** (natural logarithms of the true probabilities), which is why they appear negative. What matters is **which log-probability is larger** (i.e. closer to zero).  \n",
    "\n",
    "Given the output:\n",
    "\n",
    "```python \n",
    "(1, {0: -5.209121787743566, 1: -4.102643365036796})\n",
    "```\n",
    "\n",
    "- Log-probability for class 0: **–5.2091**  \n",
    "- Log-probability for class 1: **–4.1026**  \n",
    "\n",
    "Since  \n",
    "$$\n",
    "-4.1026 > -5.2091\n",
    "$$\n",
    "class 1 has the higher log-probability, so the model predicts **1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's check with the model already created by SKLearn if the prediction is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alejo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but CategoricalNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modelo = CategoricalNB()\n",
    "modelo.fit(X,y)\n",
    "modelo.predict([[1,0 , 1 , 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the SKLearn model predicted the same thing as our Manual version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Make some new predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outlook  temp  humidity  windy  play\n",
       "0        2     1         0      0     0\n",
       "1        1     2         0      1     1\n",
       "2        0     0         1      0     1\n",
       "3        2     2         1      1     1\n",
       "4        1     0         1      0     1\n",
       "5        0     1         0      1     1\n",
       "6        2     0         0      1     0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = {\n",
    "    'outlook':   ['sunny',    'rainy',    'overcast', 'sunny',    'rainy',    'overcast', 'sunny'],\n",
    "    'temp':      ['hot',      'mild',     'cool',     'mild',     'cool',     'hot',      'cool'],\n",
    "    'humidity':  ['high',     'high',     'normal',   'normal',   'normal',   'high',     'high'],\n",
    "    'windy':     [False,      True,       False,      True,       False,      True,       True],\n",
    "    'play':      ['no',       'yes',      'yes',      'yes',      'yes',      'yes',      'no']}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "for col, le in encoders.items():\n",
    "    if new_df[col].dtype == 'bool':\n",
    "        new_df[col] = new_df[col].astype(int)\n",
    "    new_df[col] = le.transform(new_df[col])\n",
    "\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {0: -3.8873659477612463, 1: -4.6780075099403575})\n",
      "(0, {0: -3.8873659477612463, 1: -4.439115601658008})\n",
      "(1, {0: -6.595416148863456, 1: -3.3198840257871636})\n",
      "(1, {0: -4.515974607183621, 1: -4.167181886174366})\n",
      "(1, {0: -5.496803860195347, 1: -3.5430275771013733})\n",
      "(1, {0: -4.9859782364293554, 1: -4.726797674109789})\n",
      "(0, {0: -4.005148983417629, 1: -4.9499412254239985})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = [i for i in new_df if i != \"play\"]\n",
    "X_test = new_df[labels]\n",
    "\n",
    "predicciones=[]\n",
    "for i in range(X_test.shape[0]):\n",
    "  pr = model.predict(list(X_test.iloc[i , :]))\n",
    "  print(pr)\n",
    "  predicciones.append(pr[0])\n",
    "\n",
    "new_df['Naive'] = predicciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "      <th>Naive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outlook  temp  humidity  windy  play  Naive\n",
       "0        2     1         0      0     0      0\n",
       "1        1     2         0      1     1      0\n",
       "2        0     0         1      0     1      1\n",
       "3        2     2         1      1     1      1\n",
       "4        1     0         1      0     1      1\n",
       "5        0     1         0      1     1      1\n",
       "6        2     0         0      1     0      0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Overall accuracy:** 6 out of 7 correct → ~85.7%.  \n",
    "- **True Negatives (TN):** 2 (rows 0, 6: play=0, predicted=0)  \n",
    "- **True Positives (TP):** 4 (rows 2–5: play=1, predicted=1)  \n",
    "- **False Negatives (FN):** 1 (row 1: play=1, predicted=0)  \n",
    "- **False Positives (FP):** 0  \n",
    "\n",
    "The only mistake is at index 1, where the model predicted 0 but the true label was 1 (a false negative). All other cases are correct. This tells us the classifier is quite accurate on this small set, with perfect specificity (no false alarms) and a single miss on a positive example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do the same predictions with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modelo = CategoricalNB()\n",
    "modelo.fit(X,y)\n",
    "modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The SciKit-Learn `CategoricalNB` model produces the exact same predictions as our manual implementation:\n",
    "\n",
    "```python \n",
    "array([0, 0, 1, 1, 1, 1, 0])\n",
    "```\n",
    "\n",
    "\n",
    "This perfect match confirms that our `NaiveBayes` class correctly reproduces sklearn’s behavior on this dataset.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Extend to Numerical Features**  \n",
    "   - Implement **Gaussian Naive Bayes** for continuous variables.  \n",
    "   - Or add a **discretization** step to convert numerics into categories before fitting.\n",
    "\n",
    "2. **Enrich the API**  \n",
    "   - Add `predict_proba()` to return class probabilities.  \n",
    "   - Support `score()` for quick accuracy checks.  \n",
    "   - Expose hyperparameters (`alpha`, `priors`) in the constructor.\n",
    "\n",
    "3. **Performance and Robustness**  \n",
    "   - **Vectorize** loops with NumPy for faster training/prediction.  \n",
    "   - Add **input validation** and handle unseen categories gracefully.  \n",
    "\n",
    "4. **Pipeline Compatibility**  \n",
    "   - Subclass `BaseEstimator` and `ClassifierMixin` for full scikit-learn compatibility.  \n",
    "   - Enable use within `Pipeline` alongside encoders and scalers.\n",
    "\n",
    "By following these steps, you can evolve this script into a production-ready Naive Bayes classifier—handling both categorical and numerical data, offering a rich API, and fitting seamlessly into common data-science workflows.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
